{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# ETL - Extração, Transformação e Carregamento de Dados no BigQuery\n",
        "\n",
        "Este notebook realiza o processo completo de ETL (Extract, Transform, Load) para carregar dados de diferentes fontes externas no Google BigQuery.\n",
        "\n",
        "## Visão Geral do Processo\n",
        "\n",
        "O pipeline processa quatro datasets principais:\n",
        "1. **Pedidos**: Dados de transações em formato JSON comprimido\n",
        "2. **Consumidores**: Informações de usuários em CSV comprimido\n",
        "3. **Restaurantes**: Dados de estabelecimentos em CSV comprimido\n",
        "4. **Teste A/B**: Dados de experimentos em arquivo TAR comprimido\n",
        "\n",
        "## Estrutura do Notebook\n",
        "\n",
        "1. **Instalação de Dependências**: Configuração do ambiente\n",
        "2. **Autenticação**: Acesso ao Google Cloud Platform\n",
        "3. **Configuração Inicial**: Imports e parâmetros do projeto\n",
        "4. **ETL Pedidos**: Processamento em chunks para otimização de memória\n",
        "5. **ETL Consumidores**: Carregamento direto de dados estruturados\n",
        "6. **ETL Restaurantes**: Processamento de dados de estabelecimentos\n",
        "7. **ETL Teste A/B**: Extração e carregamento de dados experimentais\n",
        "8. **Validação**: Verificação dos dados carregados\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instalacao"
      },
      "source": [
        "## 1. Instalação de Dependências\n",
        "\n",
        "Instalamos os pacotes necessários para o processo de ETL:\n",
        "\n",
        "- **pandas**: Manipulação e análise de dados\n",
        "- **pandas-gbq**: Interface simplificada para BigQuery\n",
        "- **pyarrow**: Engine otimizado para operações com DataFrames\n",
        "\n",
        "Estes pacotes fornecem as ferramentas essenciais para extração, transformação e carregamento eficiente dos dados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "install_packages"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet pandas pandas-gbq pyarrow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "autenticacao"
      },
      "source": [
        "## 2. Autenticação no Google Cloud\n",
        "\n",
        "A autenticação é necessária para acessar e escrever dados no BigQuery. O Google Colab fornece uma interface simplificada que gerencia automaticamente as credenciais."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "auth_google",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af59502e-bc06-4ae8-afb1-c9e7ca37638b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Autenticação realizada com sucesso!\n"
          ]
        }
      ],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "print(\"Autenticação realizada com sucesso!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "configuracao"
      },
      "source": [
        "## 3. Configuração Inicial\n",
        "\n",
        "Importamos as bibliotecas necessárias e configuramos os parâmetros do projeto:\n",
        "\n",
        "- **pandas**: Manipulação de dados\n",
        "- **urllib.request**: Download de arquivos\n",
        "- **gzip/tarfile**: Descompressão de arquivos\n",
        "- **google.cloud.bigquery**: Cliente nativo do BigQuery\n",
        "- **pandas_gbq**: Interface pandas para BigQuery"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "imports_config",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f5eac0d-c16d-4e21-b037-5c2a24552b3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Projeto configurado: 294185590533\n",
            "Dataset de destino: ifood\n",
            "Timestamp de execução: 2025-07-03 19:12:26\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import urllib.request\n",
        "import gzip\n",
        "import tarfile\n",
        "from google.cloud import bigquery\n",
        "import pandas_gbq\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# Configurações do projeto\n",
        "project_id = \"294185590533\"\n",
        "dataset_id = \"ifood\"\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "print(f\"Projeto configurado: {project_id}\")\n",
        "print(f\"Dataset de destino: {dataset_id}\")\n",
        "print(f\"Timestamp de execução: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "funcoes_auxiliares"
      },
      "source": [
        "## 4. Funções Auxiliares\n",
        "\n",
        "Definimos funções reutilizáveis para otimizar o processo de ETL e fornecer feedback detalhado sobre o progresso."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "helper_functions"
      },
      "outputs": [],
      "source": [
        "def download_arquivo(url, nome_arquivo):\n",
        "    \"\"\"\n",
        "    Faz download de um arquivo da URL especificada\n",
        "\n",
        "    Args:\n",
        "        url (str): URL do arquivo para download\n",
        "        nome_arquivo (str): Nome local para salvar o arquivo\n",
        "    \"\"\"\n",
        "    print(f\"Iniciando download: {nome_arquivo}\")\n",
        "    urllib.request.urlretrieve(url, nome_arquivo)\n",
        "\n",
        "    # Verifica o tamanho do arquivo baixado\n",
        "    tamanho_mb = os.path.getsize(nome_arquivo) / (1024 * 1024)\n",
        "    print(f\"Download concluído: {nome_arquivo} ({tamanho_mb:.2f} MB)\")\n",
        "\n",
        "\n",
        "def carregar_para_bigquery(dataframe, tabela_destino, modo=\"replace\"):\n",
        "    \"\"\"\n",
        "    Carrega um DataFrame para o BigQuery com tratamento de erros\n",
        "\n",
        "    Args:\n",
        "        dataframe (pd.DataFrame): DataFrame a ser carregado\n",
        "        tabela_destino (str): Nome da tabela de destino\n",
        "        modo (str): Modo de carregamento ('replace' ou 'append')\n",
        "    \"\"\"\n",
        "    try:\n",
        "        pandas_gbq.to_gbq(\n",
        "            dataframe,\n",
        "            destination_table=f\"{dataset_id}.{tabela_destino}\",\n",
        "            project_id=project_id,\n",
        "            if_exists=modo\n",
        "        )\n",
        "        print(f\"Sucesso: {len(dataframe):,} registros carregados em {tabela_destino}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao carregar {tabela_destino}: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "def verificar_tabela(nome_tabela):\n",
        "    \"\"\"\n",
        "    Verifica informações básicas de uma tabela no BigQuery\n",
        "\n",
        "    Args:\n",
        "        nome_tabela (str): Nome da tabela para verificação\n",
        "    \"\"\"\n",
        "    query = f\"\"\"\n",
        "    SELECT\n",
        "        COUNT(*) as total_registros,\n",
        "        COUNT(DISTINCT *) as registros_unicos\n",
        "    FROM `{project_id}.{dataset_id}.{nome_tabela}`\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        resultado = client.query(query).to_dataframe()\n",
        "        total = resultado.iloc[0]['total_registros']\n",
        "        unicos = resultado.iloc[0]['registros_unicos']\n",
        "        print(f\"Tabela {nome_tabela}: {total:,} registros ({unicos:,} únicos)\")\n",
        "        return total, unicos\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao verificar {nome_tabela}: {str(e)}\")\n",
        "        return None, None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etl_pedidos"
      },
      "source": [
        "## 5. ETL - Dados de Pedidos (order.json.gz)\n",
        "\n",
        "Os dados de pedidos são o maior dataset e requerem processamento especial:\n",
        "\n",
        "### Características dos Dados\n",
        "- **Formato**: JSON Lines comprimido com gzip\n",
        "- **Volume**: Aproximadamente 3.67 milhões de registros\n",
        "- **Estratégia**: Processamento em chunks para otimização de memória\n",
        "\n",
        "### Processamento em Chunks\n",
        "- **Tamanho do chunk**: 100.000 registros por vez\n",
        "- **Vantagem**: Evita sobrecarga de memória\n",
        "- **Modo de carregamento**: Replace no primeiro chunk, append nos demais"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "etl_orders_download",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4465efb0-e9ae-4376-9827-03f6179d5284"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iniciando download: order.json.gz\n",
            "Download concluído: order.json.gz (1536.18 MB)\n"
          ]
        }
      ],
      "source": [
        "# URL e configurações para dados de pedidos\n",
        "orders_url = \"https://data-architect-test-source.s3-sa-east-1.amazonaws.com/order.json.gz\"\n",
        "orders_file = \"order.json.gz\"\n",
        "orders_table = \"orders_raw\"\n",
        "\n",
        "# Download do arquivo\n",
        "download_arquivo(orders_url, orders_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "etl_orders_process",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee2a6479-00cd-4d6a-982b-102c74d41490"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iniciando processamento de order.json.gz em chunks de 100,000 registros\n",
            "============================================================\n",
            "Processando chunk 1: 100,000 registros\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 471.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sucesso: 100,000 registros carregados em orders_raw\n",
            "Total acumulado: 100,000 registros\n",
            "----------------------------------------\n",
            "Processando chunk 2: 100,000 registros\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 4415.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sucesso: 100,000 registros carregados em orders_raw\n",
            "Total acumulado: 200,000 registros\n",
            "----------------------------------------\n",
            "Processando chunk 3: 100,000 registros\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 9000.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sucesso: 100,000 registros carregados em orders_raw\n",
            "Total acumulado: 300,000 registros\n",
            "----------------------------------------\n",
            "Processando chunk 4: 100,000 registros\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 2310.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sucesso: 100,000 registros carregados em orders_raw\n",
            "Total acumulado: 400,000 registros\n",
            "----------------------------------------\n",
            "Processando chunk 5: 100,000 registros\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 4871.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sucesso: 100,000 registros carregados em orders_raw\n",
            "Total acumulado: 500,000 registros\n",
            "----------------------------------------\n",
            "Processando chunk 6: 100,000 registros\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 4539.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sucesso: 100,000 registros carregados em orders_raw\n",
            "Total acumulado: 600,000 registros\n",
            "----------------------------------------\n",
            "Processando chunk 7: 100,000 registros\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 2686.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sucesso: 100,000 registros carregados em orders_raw\n",
            "Total acumulado: 700,000 registros\n",
            "----------------------------------------\n",
            "Processando chunk 8: 100,000 registros\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 2489.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sucesso: 100,000 registros carregados em orders_raw\n",
            "Total acumulado: 800,000 registros\n",
            "----------------------------------------\n",
            "Processando chunk 9: 100,000 registros\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 2542.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sucesso: 100,000 registros carregados em orders_raw\n",
            "Total acumulado: 900,000 registros\n",
            "----------------------------------------\n",
            "Processando chunk 10: 100,000 registros\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 2542.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sucesso: 100,000 registros carregados em orders_raw\n",
            "Total acumulado: 1,000,000 registros\n",
            "----------------------------------------\n",
            "Processando chunk 11: 100,000 registros\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 10922.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sucesso: 100,000 registros carregados em orders_raw\n",
            "Total acumulado: 1,100,000 registros\n",
            "----------------------------------------\n",
            "Processando chunk 12: 100,000 registros\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 9939.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sucesso: 100,000 registros carregados em orders_raw\n",
            "Total acumulado: 1,200,000 registros\n",
            "----------------------------------------\n",
            "Processando chunk 13: 100,000 registros\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 2209.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sucesso: 100,000 registros carregados em orders_raw\n",
            "Total acumulado: 1,300,000 registros\n",
            "----------------------------------------\n",
            "Processando chunk 14: 100,000 registros\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 2657.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sucesso: 100,000 registros carregados em orders_raw\n",
            "Total acumulado: 1,400,000 registros\n",
            "----------------------------------------\n",
            "Processando chunk 15: 100,000 registros\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 5140.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sucesso: 100,000 registros carregados em orders_raw\n",
            "Total acumulado: 1,500,000 registros\n",
            "----------------------------------------\n",
            "Processando chunk 16: 100,000 registros\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 2444.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sucesso: 100,000 registros carregados em orders_raw\n",
            "Total acumulado: 1,600,000 registros\n",
            "----------------------------------------\n",
            "Processando chunk 17: 100,000 registros\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 2597.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sucesso: 100,000 registros carregados em orders_raw\n",
            "Total acumulado: 1,700,000 registros\n",
            "----------------------------------------\n",
            "Processando chunk 18: 100,000 registros\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 4433.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sucesso: 100,000 registros carregados em orders_raw\n",
            "Total acumulado: 1,800,000 registros\n",
            "----------------------------------------\n",
            "Processando chunk 19: 100,000 registros\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 10951.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sucesso: 100,000 registros carregados em orders_raw\n",
            "Total acumulado: 1,900,000 registros\n",
            "----------------------------------------\n",
            "Processando chunk 20: 100,000 registros\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 7869.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sucesso: 100,000 registros carregados em orders_raw\n",
            "Total acumulado: 2,000,000 registros\n",
            "----------------------------------------\n",
            "Processando chunk 21: 100,000 registros\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 12446.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sucesso: 100,000 registros carregados em orders_raw\n",
            "Total acumulado: 2,100,000 registros\n",
            "----------------------------------------\n",
            "Processando chunk 22: 100,000 registros\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 10255.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sucesso: 100,000 registros carregados em orders_raw\n",
            "Total acumulado: 2,200,000 registros\n",
            "----------------------------------------\n",
            "Processando chunk 23: 100,000 registros\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 2442.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sucesso: 100,000 registros carregados em orders_raw\n",
            "Total acumulado: 2,300,000 registros\n",
            "----------------------------------------\n",
            "Processando chunk 24: 100,000 registros\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 11335.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sucesso: 100,000 registros carregados em orders_raw\n",
            "Total acumulado: 2,400,000 registros\n",
            "----------------------------------------\n",
            "Processando chunk 25: 100,000 registros\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 2502.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sucesso: 100,000 registros carregados em orders_raw\n",
            "Total acumulado: 2,500,000 registros\n",
            "----------------------------------------\n",
            "Processando chunk 26: 100,000 registros\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 9177.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sucesso: 100,000 registros carregados em orders_raw\n",
            "Total acumulado: 2,600,000 registros\n",
            "----------------------------------------\n",
            "Processando chunk 27: 100,000 registros\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 10754.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sucesso: 100,000 registros carregados em orders_raw\n",
            "Total acumulado: 2,700,000 registros\n",
            "----------------------------------------\n",
            "Processando chunk 28: 100,000 registros\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 12372.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sucesso: 100,000 registros carregados em orders_raw\n",
            "Total acumulado: 2,800,000 registros\n",
            "----------------------------------------\n",
            "Processando chunk 29: 100,000 registros\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 8648.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sucesso: 100,000 registros carregados em orders_raw\n",
            "Total acumulado: 2,900,000 registros\n",
            "----------------------------------------\n",
            "Processando chunk 30: 100,000 registros\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 5882.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sucesso: 100,000 registros carregados em orders_raw\n",
            "Total acumulado: 3,000,000 registros\n",
            "----------------------------------------\n",
            "Processando chunk 31: 100,000 registros\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 2647.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sucesso: 100,000 registros carregados em orders_raw\n",
            "Total acumulado: 3,100,000 registros\n",
            "----------------------------------------\n",
            "Processando chunk 32: 100,000 registros\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 11650.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sucesso: 100,000 registros carregados em orders_raw\n",
            "Total acumulado: 3,200,000 registros\n",
            "----------------------------------------\n",
            "Processando chunk 33: 100,000 registros\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 10407.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sucesso: 100,000 registros carregados em orders_raw\n",
            "Total acumulado: 3,300,000 registros\n",
            "----------------------------------------\n",
            "Processando chunk 34: 100,000 registros\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 10106.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sucesso: 100,000 registros carregados em orders_raw\n",
            "Total acumulado: 3,400,000 registros\n",
            "----------------------------------------\n",
            "Processando chunk 35: 100,000 registros\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 2752.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sucesso: 100,000 registros carregados em orders_raw\n",
            "Total acumulado: 3,500,000 registros\n",
            "----------------------------------------\n",
            "Processando chunk 36: 100,000 registros\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 2208.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sucesso: 100,000 registros carregados em orders_raw\n",
            "Total acumulado: 3,600,000 registros\n",
            "----------------------------------------\n",
            "Processando chunk 37: 70,826 registros\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 2439.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sucesso: 70,826 registros carregados em orders_raw\n",
            "Total acumulado: 3,670,826 registros\n",
            "----------------------------------------\n",
            "============================================================\n",
            "Processamento de pedidos concluído: 3,670,826 registros totais\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Processamento em chunks para otimização de memória\n",
        "chunksize = 100_000\n",
        "total_registros = 0\n",
        "\n",
        "print(f\"Iniciando processamento de {orders_file} em chunks de {chunksize:,} registros\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "with gzip.open(orders_file, \"rt\", encoding=\"utf-8\") as f:\n",
        "    reader = pd.read_json(f, lines=True, chunksize=chunksize)\n",
        "\n",
        "    for i, chunk in enumerate(reader):\n",
        "        # Informações do chunk atual\n",
        "        chunk_size = len(chunk)\n",
        "        total_registros += chunk_size\n",
        "\n",
        "        print(f\"Processando chunk {i+1}: {chunk_size:,} registros\")\n",
        "\n",
        "        # Carrega para BigQuery\n",
        "        modo_carregamento = \"replace\" if i == 0 else \"append\"\n",
        "        carregar_para_bigquery(chunk, orders_table, modo_carregamento)\n",
        "\n",
        "        print(f\"Total acumulado: {total_registros:,} registros\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(f\"Processamento de pedidos concluído: {total_registros:,} registros totais\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etl_consumidores"
      },
      "source": [
        "## 6. ETL - Dados de Consumidores (consumer.csv.gz)\n",
        "\n",
        "Os dados de consumidores são estruturados e podem ser processados diretamente:\n",
        "\n",
        "### Características dos Dados\n",
        "- **Formato**: CSV comprimido com gzip\n",
        "- **Volume**: Dataset menor, carregamento direto\n",
        "- **Estratégia**: Carregamento completo em memória"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "etl_consumers",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83888a00-9f84-4727-99e0-9e5a17bb894b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processando dados de consumidores\n",
            "========================================\n",
            "Iniciando download: consumer.csv.gz\n",
            "Download concluído: consumer.csv.gz (46.25 MB)\n",
            "Carregando dados em memória...\n",
            "Dados carregados: 806,156 registros x 7 colunas\n",
            "Colunas: ['customer_id', 'language', 'created_at', 'active', 'customer_name', 'customer_phone_area', 'customer_phone_number']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 8848.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sucesso: 806,156 registros carregados em consumers_raw\n",
            "Processamento de consumidores concluído\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Configurações para dados de consumidores\n",
        "consumers_url = \"https://data-architect-test-source.s3-sa-east-1.amazonaws.com/consumer.csv.gz\"\n",
        "consumers_file = \"consumer.csv.gz\"\n",
        "consumers_table = \"consumers_raw\"\n",
        "\n",
        "print(\"Processando dados de consumidores\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Download do arquivo\n",
        "download_arquivo(consumers_url, consumers_file)\n",
        "\n",
        "# Carregamento e processamento\n",
        "print(\"Carregando dados em memória...\")\n",
        "consumers_df = pd.read_csv(consumers_file)\n",
        "\n",
        "# Informações do dataset\n",
        "print(f\"Dados carregados: {len(consumers_df):,} registros x {len(consumers_df.columns)} colunas\")\n",
        "print(f\"Colunas: {list(consumers_df.columns)}\")\n",
        "\n",
        "# Carrega para BigQuery\n",
        "carregar_para_bigquery(consumers_df, consumers_table)\n",
        "\n",
        "print(\"Processamento de consumidores concluído\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etl_restaurantes"
      },
      "source": [
        "## 7. ETL - Dados de Restaurantes (restaurant.csv.gz)\n",
        "\n",
        "Os dados de restaurantes seguem o mesmo padrão dos consumidores:\n",
        "\n",
        "### Características dos Dados\n",
        "- **Formato**: CSV comprimido com gzip\n",
        "- **Volume**: Dataset estruturado de tamanho moderado\n",
        "- **Estratégia**: Carregamento direto otimizado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "etl_restaurants",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0e34eaf-ef65-417a-818b-b663bf83e047"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processando dados de restaurantes\n",
            "========================================\n",
            "Iniciando download: restaurant.csv.gz\n",
            "Download concluído: restaurant.csv.gz (0.38 MB)\n",
            "Carregando dados em memória...\n",
            "Dados carregados: 7,292 registros x 12 colunas\n",
            "Colunas: ['id', 'created_at', 'enabled', 'price_range', 'average_ticket', 'takeout_time', 'delivery_time', 'minimum_order_value', 'merchant_zip_code', 'merchant_city', 'merchant_state', 'merchant_country']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 9446.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sucesso: 7,292 registros carregados em restaurants_raw\n",
            "Processamento de restaurantes concluído\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Configurações para dados de restaurantes\n",
        "restaurants_url = \"https://data-architect-test-source.s3-sa-east-1.amazonaws.com/restaurant.csv.gz\"\n",
        "restaurants_file = \"restaurant.csv.gz\"\n",
        "restaurants_table = \"restaurants_raw\"\n",
        "\n",
        "print(\"Processando dados de restaurantes\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Download do arquivo\n",
        "download_arquivo(restaurants_url, restaurants_file)\n",
        "\n",
        "# Carregamento e processamento\n",
        "print(\"Carregando dados em memória...\")\n",
        "restaurants_df = pd.read_csv(restaurants_file)\n",
        "\n",
        "# Informações do dataset\n",
        "print(f\"Dados carregados: {len(restaurants_df):,} registros x {len(restaurants_df.columns)} colunas\")\n",
        "print(f\"Colunas: {list(restaurants_df.columns)}\")\n",
        "\n",
        "# Carrega para BigQuery\n",
        "carregar_para_bigquery(restaurants_df, restaurants_table)\n",
        "\n",
        "print(\"Processamento de restaurantes concluído\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etl_ab_test"
      },
      "source": [
        "## 8. ETL - Dados de Teste A/B (ab_test_ref.tar.gz)\n",
        "\n",
        "Os dados de teste A/B requerem extração adicional devido ao formato TAR:\n",
        "\n",
        "### Características dos Dados\n",
        "- **Formato**: CSV dentro de arquivo TAR comprimido\n",
        "- **Processo**: Download → Extração → Carregamento\n",
        "- **Estratégia**: Extração completa e carregamento direto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZsAZEVSsQJn",
        "outputId": "f85715c3-0e51-49f3-e36e-08e60d9b61c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processando dados de teste A/B\n",
            "========================================\n",
            "Iniciando download: ab_test_ref.tar.gz\n",
            "Download concluído: ab_test_ref.tar.gz (29.78 MB)\n",
            "Extraindo arquivo TAR...\n",
            "Arquivos extraídos: ['._ab_test_ref.csv', 'ab_test_ref.csv']\n",
            "Arquivo CSV extraído: ab_test_ref.csv (55.72 MB)\n",
            "Carregando dados em memória...\n",
            "Dados carregados: 806,467 registros x 2 colunas\n",
            "Colunas: ['customer_id', 'is_target']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 2576.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sucesso: 806,467 registros carregados em ab_test_ref_raw\n",
            "Processamento de teste A/B concluído\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Configurações para dados de teste A/B\n",
        "ab_test_url = \"https://data-architect-test-source.s3-sa-east-1.amazonaws.com/ab_test_ref.tar.gz\"\n",
        "ab_test_file = \"ab_test_ref.tar.gz\"\n",
        "ab_test_csv = \"ab_test_ref.csv\"\n",
        "ab_test_table = \"ab_test_ref_raw\"\n",
        "\n",
        "print(\"Processando dados de teste A/B\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Download do arquivo\n",
        "download_arquivo(ab_test_url, ab_test_file)\n",
        "\n",
        "# Extração do arquivo TAR\n",
        "print(\"Extraindo arquivo TAR...\")\n",
        "with tarfile.open(ab_test_file, \"r:gz\") as tar:\n",
        "    tar.extractall()\n",
        "    print(f\"Arquivos extraídos: {tar.getnames()}\")\n",
        "\n",
        "# Verificação do arquivo extraído\n",
        "if os.path.exists(ab_test_csv):\n",
        "    tamanho_csv = os.path.getsize(ab_test_csv) / (1024 * 1024)\n",
        "    print(f\"Arquivo CSV extraído: {ab_test_csv} ({tamanho_csv:.2f} MB)\")\n",
        "else:\n",
        "    print(f\"Erro: Arquivo {ab_test_csv} não encontrado após extração\")\n",
        "    raise FileNotFoundError(f\"Arquivo {ab_test_csv} não foi extraído corretamente\")\n",
        "\n",
        "# Carregamento e processamento\n",
        "print(\"Carregando dados em memória...\")\n",
        "ab_test_df = pd.read_csv(ab_test_csv)\n",
        "\n",
        "# Informações do dataset\n",
        "print(f\"Dados carregados: {len(ab_test_df):,} registros x {len(ab_test_df.columns)} colunas\")\n",
        "print(f\"Colunas: {list(ab_test_df.columns)}\")\n",
        "\n",
        "# Carrega para BigQuery\n",
        "carregar_para_bigquery(ab_test_df, ab_test_table)\n",
        "\n",
        "print(\"Processamento de teste A/B concluído\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "validacao"
      },
      "source": [
        "## 9. Validação dos Dados Carregados\n",
        "\n",
        "Verificamos se todos os dados foram carregados corretamente no BigQuery, incluindo contagens de registros e verificações de integridade."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "validation",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d79082cd-500e-4543-ca19-b7fa494516b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDAÇÃO DOS DADOS CARREGADOS\n",
            "==================================================\n",
            "Verificando tabela: orders_raw\n",
            "Erro ao verificar orders_raw: 400 COUNT(*) cannot be used with DISTINCT at [4:9]; reason: invalidQuery, location: query, message: COUNT(*) cannot be used with DISTINCT at [4:9]\n",
            "\n",
            "Location: US\n",
            "Job ID: 73048739-0408-4d9c-a0b9-44f37f52be7f\n",
            "\n",
            "------------------------------\n",
            "Verificando tabela: consumers_raw\n",
            "Erro ao verificar consumers_raw: 400 COUNT(*) cannot be used with DISTINCT at [4:9]; reason: invalidQuery, location: query, message: COUNT(*) cannot be used with DISTINCT at [4:9]\n",
            "\n",
            "Location: US\n",
            "Job ID: 3ee36049-f45f-469a-a59f-74165a763db5\n",
            "\n",
            "------------------------------\n",
            "Verificando tabela: restaurants_raw\n",
            "Erro ao verificar restaurants_raw: 400 COUNT(*) cannot be used with DISTINCT at [4:9]; reason: invalidQuery, location: query, message: COUNT(*) cannot be used with DISTINCT at [4:9]\n",
            "\n",
            "Location: US\n",
            "Job ID: ec927e99-b9d2-4f5f-beee-359f484a3943\n",
            "\n",
            "------------------------------\n",
            "Verificando tabela: ab_test_ref_raw\n",
            "Erro ao verificar ab_test_ref_raw: 400 COUNT(*) cannot be used with DISTINCT at [4:9]; reason: invalidQuery, location: query, message: COUNT(*) cannot be used with DISTINCT at [4:9]\n",
            "\n",
            "Location: US\n",
            "Job ID: 5c1b772b-336e-418d-b36b-f9b456e764ec\n",
            "\n",
            "------------------------------\n",
            "==================================================\n",
            "RESUMO FINAL\n",
            "====================\n",
            "orders_raw: ERRO na verificação\n",
            "consumers_raw: ERRO na verificação\n",
            "restaurants_raw: ERRO na verificação\n",
            "ab_test_ref_raw: ERRO na verificação\n",
            "--------------------\n",
            "Total geral: 0 registros\n",
            "Timestamp de conclusão: 2025-07-03 19:28:05\n"
          ]
        }
      ],
      "source": [
        "print(\"VALIDAÇÃO DOS DADOS CARREGADOS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Lista de todas as tabelas criadas\n",
        "tabelas_criadas = [\n",
        "    \"orders_raw\",\n",
        "    \"consumers_raw\",\n",
        "    \"restaurants_raw\",\n",
        "    \"ab_test_ref_raw\"\n",
        "]\n",
        "\n",
        "# Verifica cada tabela\n",
        "resultados_validacao = {}\n",
        "\n",
        "for tabela in tabelas_criadas:\n",
        "    print(f\"Verificando tabela: {tabela}\")\n",
        "    total, unicos = verificar_tabela(tabela)\n",
        "    resultados_validacao[tabela] = {'total': total, 'unicos': unicos}\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Resumo consolidado\n",
        "print(\"RESUMO FINAL\")\n",
        "print(\"=\" * 20)\n",
        "\n",
        "total_geral = 0\n",
        "for tabela, dados in resultados_validacao.items():\n",
        "    if dados['total'] is not None:\n",
        "        total_geral += dados['total']\n",
        "        print(f\"{tabela}: {dados['total']:,} registros\")\n",
        "    else:\n",
        "        print(f\"{tabela}: ERRO na verificação\")\n",
        "\n",
        "print(\"-\" * 20)\n",
        "print(f\"Total geral: {total_geral:,} registros\")\n",
        "print(f\"Timestamp de conclusão: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "limpeza"
      },
      "source": [
        "## 10. Limpeza de Arquivos Temporários\n",
        "\n",
        "Remove os arquivos baixados para liberar espaço em disco, mantendo apenas os dados no BigQuery."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "cleanup",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d8dbab4-3201-4817-e32c-24f07446fb0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Limpeza de arquivos temporários\n",
            "===================================\n",
            "Removido: order.json.gz (1536.18 MB)\n",
            "Removido: consumer.csv.gz (46.25 MB)\n",
            "Removido: restaurant.csv.gz (0.38 MB)\n",
            "Removido: ab_test_ref.tar.gz (29.78 MB)\n",
            "Removido: ab_test_ref.csv (55.72 MB)\n",
            "-----------------------------------\n",
            "Espaço total liberado: 1668.32 MB\n",
            "Limpeza concluída\n"
          ]
        }
      ],
      "source": [
        "# Lista de arquivos para limpeza\n",
        "arquivos_temporarios = [\n",
        "    \"order.json.gz\",\n",
        "    \"consumer.csv.gz\",\n",
        "    \"restaurant.csv.gz\",\n",
        "    \"ab_test_ref.tar.gz\",\n",
        "    \"ab_test_ref.csv\"\n",
        "]\n",
        "\n",
        "print(\"Limpeza de arquivos temporários\")\n",
        "print(\"=\" * 35)\n",
        "\n",
        "espaco_liberado = 0\n",
        "\n",
        "for arquivo in arquivos_temporarios:\n",
        "    if os.path.exists(arquivo):\n",
        "        tamanho = os.path.getsize(arquivo) / (1024 * 1024)  # MB\n",
        "        espaco_liberado += tamanho\n",
        "        os.remove(arquivo)\n",
        "        print(f\"Removido: {arquivo} ({tamanho:.2f} MB)\")\n",
        "    else:\n",
        "        print(f\"Não encontrado: {arquivo}\")\n",
        "\n",
        "print(\"-\" * 35)\n",
        "print(f\"Espaço total liberado: {espaco_liberado:.2f} MB\")\n",
        "print(\"Limpeza concluída\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instrucoes_uso"
      },
      "source": [
        "## 11. Instruções de Uso e Próximos Passos\n",
        "\n",
        "### Dados Disponíveis no BigQuery\n",
        "\n",
        "Após a execução bem-sucedida deste notebook, os seguintes datasets estarão disponíveis:\n",
        "\n",
        "1. **`orders_raw`**: Dados de pedidos (~3.67M registros)\n",
        "2. **`consumers_raw`**: Informações de consumidores\n",
        "3. **`restaurants_raw`**: Dados de restaurantes\n",
        "4. **`ab_test_ref_raw`**: Dados de experimentos A/B\n",
        "\n",
        "### Como Acessar os Dados\n",
        "\n",
        "```sql\n",
        "-- Exemplo de consulta\n",
        "SELECT COUNT(*) as total_pedidos\n",
        "FROM `294185590533.ifood.orders_raw`\n",
        "```\n",
        "\n",
        "### Próximos Passos Recomendados\n",
        "\n",
        "1. **Análise Exploratória**: Use o notebook de EDA para entender os dados\n",
        "2. **Modelagem de Dados**: Crie tabelas dimensionais e de fatos\n",
        "3. **Análises de Negócio**: Desenvolva métricas e KPIs\n",
        "4. **Automação**: Configure pipelines para atualizações regulares\n",
        "\n",
        "### Monitoramento\n",
        "\n",
        "- Verifique regularmente a integridade dos dados\n",
        "- Monitore o crescimento das tabelas\n",
        "- Implemente alertas para falhas no pipeline"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}